# 數據資料的爬取
- 讀取、解析、自動化

----

# requests

### GET 請求
- 
    ```python
    import requests
    response物件 = request.get(網址)

    request.text # 文字檔案資料
    request.content # 二進位檔案資料
    status_code # HTTP 的狀態碼
    ```
    > 如果 HTTP 狀態碼為 `200` 或 `requests.codes.ok` 就代表內容取得成功。
- 加上 URL 參數
  1. 網址`?`參數
  2. 參數`=`值
  3. 參數`&`參數
  4. 
    ```python 
    import requests

    payliad = {
        "key1":"value1",
        "key2":"value2"
    }

    req = request.get("網址", params = payload) # 加上 URL 參數
    ```
- 測試 `request` 和 `response` 的網站
  - [httpbin.org - get](http://httpbin.org/get)
  - [httpbin.org - post](http://httpbin.org/post)
    1. 對網站發出 `GET`、`POST` 
    2. 以 json 格式回應：`args`、`headers`、`origin`(來源網址)、`url`(請求網址)
    3. [httpbin.org](http://httpbin.org)可以接收所有 HTTP 的傳送方式




### POST 請求
- 
    ```python 
    import requests

    payliad = {
        "key1":"value1",
        "key2":"value2"
    }

    req = request.post("網址", data = payload) # 加上 POST 傳遞的參數
    ```



> 比較
> |請求種類|參數型態|參數|
> |:---:|:---:|:---:|
> |`requests.get` 中 URL 的參數|字典|`params = payload`|
> |`requests.post` 傳遞的參數|字典|`data = payload`|




### HTTP Headers 
- 
    ```python
    import requests

    url = "網址"

    headers = {
        "user-agent":".....", 
        "cookie":"...", 
        ...
    }

    req = requests.get(url, headers = headers)
    ```
- 若 `print(req)` 得到 HTTP 狀態碼為 `200` 表示正確讀取。
    ```python
    <Response [200]>
    ```




### Session & Cookie
- [白話 Session 與 Cookie：從經營雜貨店開始](https://hulitw.medium.com/session-and-cookie-15e47ed838bc)
- 利用 cookie 檢查篩選使用者

----

# BeautifulSoup

### 網頁架構
![網頁基本架構](https://github.com/49831117/Essay/blob/master/image/html.jpg)


### bs4 


```python
from bs4 import BeautifulSoup
bs = BeautifulSoup(原始碼, 解析器)
```


1. 常用屬性

2. 常用方法
`find()`、`find_all()`、`select()`
3. 取得標籤屬性內容
> **Side project**
> 
> []()

### 使用正規表達式

----

# Selenium
